{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6d69b6-d192-4989-91f6-746eb09e049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d83e6ab-83fa-43c2-9f73-3443b41d0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import pathlib\n",
    "import argparse\n",
    "import copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ad34ff-3945-47d6-b688-2cb14b8bdb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e13960e3-e0d6-42c7-9b1f-6560810d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs=pd.read_csv(\"Meta-Llama-3-8B-Instruct.csv\").references.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ef6b49-768e-4d2e-ad1a-05039609950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "XQuAD\n",
    "\n",
    "Official evaluation script for v1.1 of the SQuAD dataset. \"\"\"\n",
    "# fork from https://huggingface.co/spaces/evaluate-metric/squad/blob/main/squad.py\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def cut_thai(text): # Add!!!\n",
    "        return ' '.join(text.split())#word_tokenize(text,engine=\"newmm\"))  # For Thai\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(cut_thai(s)))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def compute_score(references, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "    for _ground_truths,_prediction in list(zip(references, predictions)):\n",
    "        total += 1\n",
    "        ground_truths = [_ground_truths]\n",
    "        prediction = _prediction\n",
    "        exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\n",
    "\n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {\"exact_match\": exact_match, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff44308c-6bac-446e-ae1f-a41cfd5672ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da6dd843a28415b804c603e5c4ba86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a705a1eac4b423d96649ee3598277d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.07 seconds, 93.82 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"WangchanLion7B.csv\")\n",
    "_temp=pd.read_csv(\"./result/eval-gpt-4-WangchanLion7B.csv\")\n",
    "cands=df.predictions.tolist()\n",
    "df[\"q1\"]=_temp[\"model_q1\"]\n",
    "df[\"q2\"]=_temp[\"model_q2\"]\n",
    "df[\"q3\"]=_temp[\"model_q3\"]\n",
    "df[\"q4\"]=_temp[\"model_q4\"]\n",
    "P, R, F1 = score(cands, refs, lang='th', verbose=True)\n",
    "df[\"bertscore_p\"]=[float(i)*100 for i in P]\n",
    "df[\"bertscore_r\"]=[float(i)*100 for i in R]\n",
    "df[\"bertscore_f1\"]=[float(i)*100 for i in F1]\n",
    "exact_match=[]\n",
    "f1=[]\n",
    "for r,p in zip(df[\"references\"],df[\"predictions\"]):\n",
    "    _temp = compute_score([r.strip()],[p.strip()])\n",
    "    exact_match.append(_temp['exact_match'])\n",
    "    f1.append(_temp['f1'])\n",
    "df[\"xquad_exact_match\"]=exact_match\n",
    "df[\"xquad_f1\"]=f1\n",
    "df.to_excel(\"mbert-done_eval_WangchanLion7B.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d429ca-14e8-4230-b586-f77c22ebdaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5642b882814940a163b61d36da6f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400f67e6c69d4eb8bf42d3e62d2eb5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.41 seconds, 241.61 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"llama3-8b-cpt-sea-lionv2-instruct.csv\")\n",
    "_temp=pd.read_csv(\"./result/eval-gpt-4-llama3-8b-cpt-sea-lionv2-instruct.csv\")\n",
    "cands=df.predictions.tolist()\n",
    "df[\"q1\"]=_temp[\"model_q1\"]\n",
    "df[\"q2\"]=_temp[\"model_q2\"]\n",
    "df[\"q3\"]=_temp[\"model_q3\"]\n",
    "df[\"q4\"]=_temp[\"model_q4\"]\n",
    "P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "df[\"bertscore_p\"]=[float(i)*100 for i in P]\n",
    "df[\"bertscore_r\"]=[float(i)*100 for i in R]\n",
    "df[\"bertscore_f1\"]=[float(i)*100 for i in F1]\n",
    "exact_match=[]\n",
    "f1=[]\n",
    "for r,p in zip(df[\"references\"],df[\"predictions\"]):\n",
    "    _temp = compute_score([r.strip()],[p.strip()])\n",
    "    exact_match.append(_temp['exact_match'])\n",
    "    f1.append(_temp['f1'])\n",
    "df[\"xquad_exact_match\"]=exact_match\n",
    "df[\"xquad_f1\"]=f1\n",
    "df.to_excel(\"mbert-done_eval_llama3-8b-cpt-sea-lionv2-instruct.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62a1d37-d231-4b19-83cc-96a1646bbaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40612507d9b447b2a948482f642144a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a68da7232324e6e965bb54e013548e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.59 seconds, 169.20 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Meta-Llama-3.1-8B-Instruct.csv\")\n",
    "_temp=pd.read_csv(\"./result/eval-gpt-4-Meta-Llama-3.1-8B-Instruct.csv\")\n",
    "cands=df.predictions.tolist()\n",
    "df[\"q1\"]=_temp[\"model_q1\"]\n",
    "df[\"q2\"]=_temp[\"model_q2\"]\n",
    "df[\"q3\"]=_temp[\"model_q3\"]\n",
    "df[\"q4\"]=_temp[\"model_q4\"]\n",
    "P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "df[\"bertscore_p\"]=[float(i)*100 for i in P]\n",
    "df[\"bertscore_r\"]=[float(i)*100 for i in R]\n",
    "df[\"bertscore_f1\"]=[float(i)*100 for i in F1]\n",
    "exact_match=[]\n",
    "f1=[]\n",
    "for r,p in zip(df[\"references\"],df[\"predictions\"]):\n",
    "    _temp = compute_score([r.strip()],[p.strip()])\n",
    "    exact_match.append(_temp['exact_match'])\n",
    "    f1.append(_temp['f1'])\n",
    "df[\"xquad_exact_match\"]=exact_match\n",
    "df[\"xquad_f1\"]=f1\n",
    "df.to_excel(\"mbert-done_eval_Meta-Llama-3.1-8B-Instruct.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c6aa59-2a21-4b2e-a9c2-649408acd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00258682bb77452fb571a17e4f4df55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b51305c445418b831a6a90f26cd203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.62 seconds, 161.28 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"Meta-Llama-3-8B-Instruct.csv\")\n",
    "_temp=pd.read_csv(\"./result/eval-gpt-4-Meta-Llama-3-8B-Instruct.csv\")\n",
    "cands=df.predictions.tolist()\n",
    "df[\"q1\"]=_temp[\"model_q1\"]\n",
    "df[\"q2\"]=_temp[\"model_q2\"]\n",
    "df[\"q3\"]=_temp[\"model_q3\"]\n",
    "df[\"q4\"]=_temp[\"model_q4\"]\n",
    "P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "df[\"bertscore_p\"]=[float(i)*100 for i in P]\n",
    "df[\"bertscore_r\"]=[float(i)*100 for i in R]\n",
    "df[\"bertscore_f1\"]=[float(i)*100 for i in F1]\n",
    "exact_match=[]\n",
    "f1=[]\n",
    "for r,p in zip(df[\"references\"],df[\"predictions\"]):\n",
    "    _temp = compute_score([r.strip()],[p.strip()])\n",
    "    exact_match.append(_temp['exact_match'])\n",
    "    f1.append(_temp['f1'])\n",
    "df[\"xquad_exact_match\"]=exact_match\n",
    "df[\"xquad_f1\"]=f1\n",
    "df.to_excel(\"mbert-done_eval_Meta-Llama-3-8B-Instruct.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6afe1be2-6fc2-493a-abb0-fbd45f7513f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e745d813912a4260a182fa0467cd5ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131f03eb90a14365864e5c4de16acc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.67 seconds, 37.48 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"openthaigpt-1.0.0-beta-7b-chat-ckpt-hf.csv\")\n",
    "_temp=pd.read_csv(\"./result/eval-gpt-4-openthaigpt-1.0.0-beta-7b-chat-ckpt-hf.csv\")\n",
    "cands=df.predictions.tolist()\n",
    "df[\"q1\"]=_temp[\"model_q1\"]\n",
    "df[\"q2\"]=_temp[\"model_q2\"]\n",
    "df[\"q3\"]=_temp[\"model_q3\"]\n",
    "df[\"q4\"]=_temp[\"model_q4\"]\n",
    "P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "df[\"bertscore_p\"]=[float(i)*100 for i in P]\n",
    "df[\"bertscore_r\"]=[float(i)*100 for i in R]\n",
    "df[\"bertscore_f1\"]=[float(i)*100 for i in F1]\n",
    "exact_match=[]\n",
    "f1=[]\n",
    "for r,p in zip(df[\"references\"],df[\"predictions\"]):\n",
    "    _temp = compute_score([r.strip()],[p.strip()])\n",
    "    exact_match.append(_temp['exact_match'])\n",
    "    f1.append(_temp['f1'])\n",
    "df[\"xquad_exact_match\"]=exact_match\n",
    "df[\"xquad_f1\"]=f1\n",
    "df.to_excel(\"done_eval_openthaigpt-1.0.0-beta-7b-chat-ckpt-hf.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faade171-c629-4144-bd7c-5c408926c407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221eecad451f443e95ad953de7a317b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b573b226f2884a3face73bd1f4f03dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.57 seconds, 175.46 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"SeaLLM-7B-v2.csv\")\n",
    "_temp=pd.read_csv(\"./result/eval-gpt-4-SeaLLM-7B-v2.csv\")\n",
    "cands=df.predictions.tolist()\n",
    "df[\"q1\"]=_temp[\"model_q1\"]\n",
    "df[\"q2\"]=_temp[\"model_q2\"]\n",
    "df[\"q3\"]=_temp[\"model_q3\"]\n",
    "df[\"q4\"]=_temp[\"model_q4\"]\n",
    "P, R, F1 = score(cands, refs, lang='en', verbose=True)\n",
    "df[\"bertscore_p\"]=[float(i)*100 for i in P]\n",
    "df[\"bertscore_r\"]=[float(i)*100 for i in R]\n",
    "df[\"bertscore_f1\"]=[float(i)*100 for i in F1]\n",
    "exact_match=[]\n",
    "f1=[]\n",
    "for r,p in zip(df[\"references\"],df[\"predictions\"]):\n",
    "    _temp = compute_score([r.strip()],[p.strip()])\n",
    "    exact_match.append(_temp['exact_match'])\n",
    "    f1.append(_temp['f1'])\n",
    "df[\"xquad_exact_match\"]=exact_match\n",
    "df[\"xquad_f1\"]=f1\n",
    "df.to_excel(\"done_eval_SeaLLM-7B-v2.xlsx\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
